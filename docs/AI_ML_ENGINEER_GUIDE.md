# ğŸ¤– AI/ML Engineer æŠ€èƒ½å±•ç¤ºæŒ‡å—

## ğŸ“‹ èŒä½è¦æ±‚å¯¹ç…§è¡¨

åŸºäº Monoya AI/ML Engineer JDï¼Œæœ¬é¡¹ç›®å±•ç¤ºäº†ä»¥ä¸‹æŠ€èƒ½ï¼š

| JD è¦æ±‚ | æœ¬é¡¹ç›®å®ç° | ä»£ç ä½ç½® |
|---------|-----------|---------|
| **LLM/Agents: OpenAI, Ollama** | âœ… Ollama (Llama 3.2 1B) æœ¬åœ°éƒ¨ç½² | `docker-compose.yml`, `llama_rag_service.py` |
| **Retrieval: vector DB** | âœ… Weaviate å‘é‡æ•°æ®åº“ | `backend/app/core/weaviate_client.py` |
| **Serving: FastAPI, Docker** | âœ… FastAPI + Docker + Cloud Run ready | `backend/` |
| **RAG pipelines** | âœ… å®Œæ•´çš„ RAG å®ç° | `backend/app/services/llama_rag_service.py` |
| **Embeddings & semantic search** | âœ… Ollama embeddings + Weaviate æœç´¢ | `generate_embedding()`, `search_similar_diaries()` |
| **Python clean code** | âœ… Type hints, async/await, é”™è¯¯å¤„ç† | æ•´ä¸ª backend |

---

## ğŸ§  ç¬¬ä¸€éƒ¨åˆ†ï¼šLLM é©±åŠ¨çš„ AI ç³»ç»Ÿ

### 1.1 æœ¬åœ° LLM éƒ¨ç½² (Ollama)

**èŒä½è¦æ±‚**ï¼š
> Have shipped something with modern LLM toolingâ€”OpenAI, Ollama, vLLM, Hugging Face, LangChain

**æœ¬é¡¹ç›®å®ç°**ï¼šç”Ÿäº§çº§ Ollama éƒ¨ç½²

#### Docker Compose é…ç½®

```yaml
# docker-compose.yml
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    command: serve

volumes:
  ollama_data:  # æŒä¹…åŒ–æ¨¡å‹å­˜å‚¨
```

**éƒ¨ç½²å‘½ä»¤**ï¼š
```bash
# 1. å¯åŠ¨ Ollama æœåŠ¡
docker compose up -d ollama

# 2. ä¸‹è½½æ¨¡å‹
docker exec jd_project-ollama-1 ollama pull llama3.2:1b

# 3. éªŒè¯æ¨¡å‹
docker exec jd_project-ollama-1 ollama list
```

**è¾“å‡º**ï¼š
```
NAME           ID              SIZE      MODIFIED
llama3.2:1b    baf6a787fdff    1.3 GB    5 minutes ago
```

---

#### æ¨¡å‹é€‰æ‹©ç†ç”±

| æ¨¡å‹ | å¤§å° | é€Ÿåº¦ | ç”¨é€” | ä¸ºä»€ä¹ˆé€‰æ‹© |
|------|------|------|------|-----------|
| **llama3.2:1b** | 1.3GB | 5-8s | å®æ—¶æ¨è | âœ… å¿«é€Ÿå“åº”ï¼Œä¸­æ–‡å‹å¥½ |
| llama3.2:3b | 3.8GB | 12-15s | æ·±åº¦åˆ†æ | è´¨é‡æ›´é«˜ä½†æ›´æ…¢ |
| mistral:7b | 7.2GB | 20-30s | å¤æ‚ä»»åŠ¡ | å¤ªæ…¢ä¸é€‚åˆå®æ—¶ |

**æŠ€æœ¯æƒè¡¡**ï¼š
- âœ… **1B æ¨¡å‹**: å»¶è¿Ÿ < 10sï¼Œç”¨æˆ·ä½“éªŒå¥½
- âœ… **æœ¬åœ°éƒ¨ç½²**: æ—  API æˆæœ¬ï¼Œæ•°æ®éšç§
- âœ… **å¯æ‰©å±•**: éœ€è¦æ—¶å¯æ¢æ›´å¤§æ¨¡å‹

---

### 1.2 LLM API å®¢æˆ·ç«¯å°è£…

```python
# backend/app/services/llama_rag_service.py
import httpx
from typing import List

class LlamaRAGService:
    """
    ç”Ÿäº§çº§ LLM æœåŠ¡å°è£…
    
    ç‰¹æ€§:
    - è¶…æ—¶æ§åˆ¶
    - é”™è¯¯å¤„ç†
    - ç»“æ„åŒ–æ—¥å¿—
    - æ€§èƒ½ç›‘æ§
    """
    
    def __init__(self):
        self.ollama_url = settings.ollama_url  # http://ollama:11434
        self.model = settings.ollama_model     # llama3.2:1b
        self.timeout = 60.0  # 60 ç§’è¶…æ—¶
    
    async def generate_text(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 200
    ) -> str:
        """
        è°ƒç”¨ LLM ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: æç¤ºè¯
            temperature: åˆ›é€ æ€§ (0-1)
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        
        Raises:
            TimeoutException: è¯·æ±‚è¶…æ—¶
            HTTPException: API é”™è¯¯
        """
        print(f"[LLM] ç”Ÿæˆè¯·æ±‚: {len(prompt)} å­—ç¬¦çš„æç¤ºè¯")
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,  # éæµå¼å“åº”
                        "options": {
                            "temperature": temperature,
                            "num_predict": max_tokens
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    generated_text = result.get("response", "")
                    
                    print(f"[LLM] âœ… æˆåŠŸç”Ÿæˆ: {len(generated_text)} å­—ç¬¦")
                    return generated_text
                else:
                    error_msg = f"LLM API é”™è¯¯: {response.status_code}"
                    print(f"[LLM] âŒ {error_msg}")
                    raise HTTPException(status_code=500, detail=error_msg)
                    
        except httpx.TimeoutException:
            print(f"[LLM] â±ï¸ è¯·æ±‚è¶…æ—¶ (>{self.timeout}s)")
            raise HTTPException(
                status_code=504,
                detail="LLM è¯·æ±‚è¶…æ—¶ï¼Œæ¨¡å‹å¯èƒ½æ­£åœ¨åŠ è½½"
            )
        except Exception as e:
            print(f"[LLM] âŒ æœªçŸ¥é”™è¯¯: {e}")
            raise HTTPException(status_code=500, detail=str(e))
```

**ç”Ÿäº§çº§ç‰¹æ€§**ï¼š
- âœ… **è¶…æ—¶æ§åˆ¶**: é˜²æ­¢æ— é™ç­‰å¾…
- âœ… **é”™è¯¯åˆ†ç±»**: TimeoutException vs HTTPException
- âœ… **ç»“æ„åŒ–æ—¥å¿—**: ä¾¿äºè°ƒè¯•å’Œç›‘æ§
- âœ… **ç±»å‹æç¤º**: mypy é™æ€æ£€æŸ¥
- âœ… **æ–‡æ¡£å­—ç¬¦ä¸²**: æ¸…æ™°çš„ API è¯´æ˜

---

## ğŸ” ç¬¬äºŒéƒ¨åˆ†ï¼šå‘é‡åµŒå…¥ä¸è¯­ä¹‰æœç´¢

### 2.1 Embeddings ç”Ÿæˆ

**èŒä½è¦æ±‚**ï¼š
> Vector search & knowledge graphs â€“ build and tune semantic search over Firestore + Weaviate

**æœ¬é¡¹ç›®å®ç°**ï¼šOllama embeddings + Weaviate å‘é‡æœç´¢

```python
async def generate_embedding(self, text: str) -> List[float]:
    """
    ç”Ÿæˆæ–‡æœ¬çš„å‘é‡åµŒå…¥
    
    å·¥ä½œåŸç†:
    1. å°†æ–‡æœ¬å‘é€åˆ° Ollama embeddings API
    2. æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸º 768 ç»´å‘é‡
    3. å‘é‡æ•æ‰æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯
    
    ä¸ºä»€ä¹ˆé‡è¦:
    - å‘é‡ç›¸ä¼¼åº¦ = è¯­ä¹‰ç›¸ä¼¼åº¦
    - "å¤©æ°”å¾ˆå¥½" å’Œ "é˜³å…‰æ˜åªš" ä¼šæœ‰ç›¸ä¼¼çš„å‘é‡
    - æ”¯æŒè·¨è¯­è¨€æœç´¢ï¼ˆJA-ENï¼‰
    """
    print(f"[Embeddings] ç”ŸæˆåµŒå…¥: {len(text)} å­—ç¬¦")
    
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.post(
            f"{self.ollama_url}/api/embeddings",
            json={
                "model": self.model,
                "prompt": text
            }
        )
        
        if response.status_code == 200:
            result = response.json()
            embedding = result.get("embedding", [])
            
            print(f"[Embeddings] âœ… ç”Ÿæˆ {len(embedding)} ç»´å‘é‡")
            return embedding
        else:
            print(f"[Embeddings] âŒ å¤±è´¥: {response.status_code}")
            return []
```

**å®é™…ç¤ºä¾‹**ï¼š

```python
# è¾“å…¥
text_1 = "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
text_2 = "ä»Šå¤©é˜³å…‰æ˜åªš"
text_3 = "æˆ‘è®¨åŒä¸‹é›¨å¤©"

# ç”Ÿæˆå‘é‡ï¼ˆç®€åŒ–ï¼Œå®é™…æ˜¯ 768 ç»´ï¼‰
embedding_1 = [0.12, -0.45, 0.78, 0.23, ...]
embedding_2 = [0.15, -0.42, 0.81, 0.25, ...]  # ä¸ text_1 æ¥è¿‘
embedding_3 = [-0.23, 0.67, -0.45, -0.31, ...] # ä¸ text_1 å¾ˆä¸åŒ

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
similarity(embedding_1, embedding_2) = 0.92  # é«˜ç›¸ä¼¼åº¦
similarity(embedding_1, embedding_3) = 0.15  # ä½ç›¸ä¼¼åº¦
```

---

### 2.2 Weaviate å‘é‡æ•°æ®åº“

```python
# backend/app/core/weaviate_client.py
import weaviate

def get_weaviate_client():
    """
    åˆå§‹åŒ– Weaviate å®¢æˆ·ç«¯
    
    Weaviate ä¼˜åŠ¿:
    - ä¸“é—¨ä¸ºå‘é‡æœç´¢ä¼˜åŒ–
    - æ”¯æŒ HNSW ç´¢å¼•ï¼ˆå¿«é€Ÿæœ€è¿‘é‚»æœç´¢ï¼‰
    - å†…ç½®è¿‡æ»¤å’Œèšåˆ
    - GraphQL API
    """
    client = weaviate.Client(
        url=settings.weaviate_url,
        timeout_config=(5, 15)  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
    )
    
    # åˆ›å»º Schemaï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    try:
        client.schema.get("DiaryEntry")
    except:
        schema = {
            "class": "DiaryEntry",
            "vectorizer": "none",  # ä½¿ç”¨è‡ªå®šä¹‰å‘é‡
            "properties": [
                {
                    "name": "diaryId",
                    "dataType": ["string"],
                    "description": "æ—¥è®° ID"
                },
                {
                    "name": "userId",
                    "dataType": ["string"],
                    "description": "ç”¨æˆ· ID",
                    "indexSearchable": True  # æ”¯æŒè¿‡æ»¤
                },
                {
                    "name": "title",
                    "dataType": ["text"],
                    "description": "æ—¥è®°æ ‡é¢˜"
                },
                {
                    "name": "content",
                    "dataType": ["text"],
                    "description": "æ—¥è®°å†…å®¹"
                },
                {
                    "name": "createdAt",
                    "dataType": ["string"],
                    "description": "åˆ›å»ºæ—¶é—´"
                }
            ]
        }
        client.schema.create_class(schema)
        print("[Weaviate] âœ… Schema åˆ›å»ºæˆåŠŸ")
    
    return client
```

---

### 2.3 ç´¢å¼•ä¸æœç´¢

```python
async def index_diary(
    self,
    diary_id: str,
    user_id: str,
    title: str,
    content: str,
    created_at: str
):
    """
    å°†æ—¥è®°ç´¢å¼•åˆ° Weaviate
    
    æµç¨‹:
    1. åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
    2. ç”ŸæˆåµŒå…¥å‘é‡
    3. å­˜å‚¨åˆ° Weaviate
    
    æ€§èƒ½:
    - ç”ŸæˆåµŒå…¥: ~2s
    - å­˜å‚¨åˆ° Weaviate: ~100ms
    - æ€»è®¡: ~2.1s
    """
    print(f"[Index] å¼€å§‹ç´¢å¼• diary_id={diary_id}")
    
    # æ­¥éª¤ 1: åˆå¹¶æ–‡æœ¬
    full_text = f"{title}\n\n{content}"
    
    # æ­¥éª¤ 2: ç”Ÿæˆå‘é‡
    embedding = await self.generate_embedding(full_text)
    
    if not embedding:
        print(f"[Index] âš ï¸ è·³è¿‡ç´¢å¼• - æ— æ³•ç”Ÿæˆå‘é‡")
        return
    
    # æ­¥éª¤ 3: å­˜å‚¨
    self.weaviate_client.data_object.create(
        class_name="DiaryEntry",
        data_object={
            "diaryId": diary_id,
            "userId": user_id,
            "title": title,
            "content": content,
            "createdAt": created_at
        },
        vector=embedding  # 768 ç»´å‘é‡
    )
    
    print(f"[Index] âœ… ç´¢å¼•å®Œæˆ")

async def search_similar_diaries(
    self,
    user_id: str,
    query_text: str,
    limit: int = 5
) -> List[dict]:
    """
    è¯­ä¹‰æœç´¢ç›¸ä¼¼æ—¥è®°
    
    ç®—æ³•: HNSW (Hierarchical Navigable Small World)
    å¤æ‚åº¦: O(log N)
    
    æ­¥éª¤:
    1. ä¸ºæŸ¥è¯¢ç”Ÿæˆå‘é‡
    2. åœ¨å‘é‡ç©ºé—´ä¸­æ‰¾æœ€è¿‘çš„ k ä¸ªé‚»å±…
    3. è¿‡æ»¤å‡ºåŒä¸€ç”¨æˆ·çš„æ—¥è®°
    4. è¿”å›ç»“æœ
    """
    print(f"[Search] æŸ¥è¯¢: {len(query_text)} å­—ç¬¦")
    
    # æ­¥éª¤ 1: æŸ¥è¯¢å‘é‡
    query_embedding = await self.generate_embedding(query_text)
    
    if not query_embedding:
        return []
    
    # æ­¥éª¤ 2 & 3: å‘é‡æœç´¢ + è¿‡æ»¤
    result = (
        self.weaviate_client.query
        .get("DiaryEntry", ["diaryId", "title", "content", "createdAt"])
        .with_near_vector({
            "vector": query_embedding,
            "certainty": 0.7  # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
        })
        .with_where({
            "path": ["userId"],
            "operator": "Equal",
            "valueString": user_id
        })
        .with_limit(limit)
        .with_additional(["certainty", "distance"])  # è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
        .do()
    )
    
    entries = result.get("data", {}).get("Get", {}).get("DiaryEntry", [])
    
    print(f"[Search] âœ… æ‰¾åˆ° {len(entries)} ä¸ªç»“æœ")
    
    # æ‰“å°ç›¸ä¼¼åº¦åˆ†æ•°
    for i, entry in enumerate(entries, 1):
        certainty = entry.get("_additional", {}).get("certainty", 0)
        print(f"  {i}. {entry['title']} - ç›¸ä¼¼åº¦: {certainty:.2f}")
    
    return entries
```

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®Œæ•´çš„ RAG Pipeline

### 3.1 RAG æ¶æ„

**èŒä½è¦æ±‚**ï¼š
> LLM-powered agents â€“ design and deploy multi-modal, tool-using agents (RAG pipelines, function-calling)

**æœ¬é¡¹ç›®å®ç°**ï¼šç«¯åˆ°ç«¯ RAG ç³»ç»Ÿ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          RAG Pipeline æ¶æ„               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥: ç”¨æˆ·å½“å‰å†™çš„æ—¥è®°
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Retrieval (æ£€ç´¢)                      â”‚
â”‚                                          â”‚
â”‚  å½“å‰æ—¥è®° â†’ Embeddings API â†’ æŸ¥è¯¢å‘é‡   â”‚
â”‚       â†“                                  â”‚
â”‚  Weaviate å‘é‡æœç´¢                       â”‚
â”‚       â†“                                  â”‚
â”‚  Top-3 ç›¸ä¼¼å†å²æ—¥è®°                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Augmented (å¢å¼º)                      â”‚
â”‚                                          â”‚
â”‚  æ„å»ºå¢å¼ºæç¤ºè¯:                         â”‚
â”‚  - ç³»ç»ŸæŒ‡ä»¤                              â”‚
â”‚  - æ£€ç´¢åˆ°çš„å†å²æ—¥è®° (ä¸Šä¸‹æ–‡)            â”‚
â”‚  - å½“å‰æ—¥è®°                              â”‚
â”‚  - ä»»åŠ¡æŒ‡ä»¤                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Generation (ç”Ÿæˆ)                     â”‚
â”‚                                          â”‚
â”‚  å¢å¼ºæç¤ºè¯ â†’ Ollama LLM â†’ ä¸ªæ€§åŒ–å»ºè®®    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡º: åŸºäºç”¨æˆ·å†å²çš„ä¸ªæ€§åŒ–å†™ä½œå»ºè®®
```

---

### 3.2 RAG å®ç°ä»£ç 

```python
async def generate_recommendation(
    self,
    user_id: str,
    current_content: str,
    current_title: str = ""
) -> str:
    """
    å®Œæ•´çš„ RAG æµç¨‹å®ç°
    
    æ—¶é—´å¤æ‚åº¦:
    - Retrieval: O(log N) - Weaviate HNSW æœç´¢
    - Augmented: O(1) - å­—ç¬¦ä¸²æ‹¼æ¥
    - Generation: O(M) - M = ç”Ÿæˆé•¿åº¦
    
    æ€»æ—¶é—´: ~10-15s (é¦–æ¬¡åŠ è½½), ~5-8s (åç»­)
    """
    print(f"[RAG] ====== æµç¨‹å¼€å§‹ ======")
    start_time = time.time()
    
    # ===== æ­¥éª¤ 1: Retrieval (æ£€ç´¢) =====
    print(f"[RAG] æ­¥éª¤ 1/3: æ£€ç´¢ç›¸å…³æ—¥è®°")
    retrieval_start = time.time()
    
    query_text = f"{current_title}\n\n{current_content}"
    similar_diaries = await self.search_similar_diaries(
        user_id=user_id,
        query_text=query_text,
        limit=3  # Top-3
    )
    
    retrieval_time = time.time() - retrieval_start
    print(f"[RAG]   æ£€ç´¢è€—æ—¶: {retrieval_time:.2f}s")
    print(f"[RAG]   æ‰¾åˆ° {len(similar_diaries)} ç¯‡ç›¸å…³æ—¥è®°")
    
    # ===== æ­¥éª¤ 2: Augmented (å¢å¼º) =====
    print(f"[RAG] æ­¥éª¤ 2/3: æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡")
    augment_start = time.time()
    
    # æ„å»ºä¸Šä¸‹æ–‡
    context = ""
    if similar_diaries:
        context = "ç”¨æˆ·çš„ç›¸å…³å†å²æ—¥è®°ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰ï¼š\n\n"
        for i, diary in enumerate(similar_diaries, 1):
            certainty = diary.get("_additional", {}).get("certainty", 0)
            context += f"ã€ç›¸å…³æ—¥è®° {i}ã€‘(ç›¸ä¼¼åº¦: {certainty:.2f})\n"
            context += f"æ ‡é¢˜: {diary.get('title', 'æ— æ ‡é¢˜')}\n"
            context += f"å†…å®¹: {diary.get('content', '')[:300]}...\n\n"
    else:
        context = "ç”¨æˆ·è¿˜æ²¡æœ‰å†å²æ—¥è®°ï¼Œè¿™æ˜¯ç¬¬ä¸€ç¯‡ã€‚\n\n"
    
    # æ„å»ºæç¤ºè¯
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ—¥è®°åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·çš„ç›¸å…³å†å²æ—¥è®°å’Œå½“å‰æ­£åœ¨å†™çš„å†…å®¹ï¼Œæä¾›æœ‰å¸®åŠ©çš„å»ºè®®ã€‚

{context}

å½“å‰æ­£åœ¨å†™çš„æ—¥è®°ï¼š
æ ‡é¢˜: {current_title}
å†…å®¹: {current_content[:500]}

è¯·æä¾›ï¼š
1. å¯¹å½“å‰å†…å®¹çš„ç®€çŸ­è¯„è®º
2. ä¸å†å²æ—¥è®°çš„è”ç³»æˆ–ä¸»é¢˜è§‚å¯Ÿ
3. 1-2æ¡å†™ä½œå»ºè®®æˆ–æ€è€ƒæ–¹å‘

ç”¨ä¸­æ–‡å›å¤ï¼Œä¿æŒæ¸©æš–å’Œé¼“åŠ±çš„è¯­æ°”ï¼Œä¸è¶…è¿‡150å­—ã€‚"""
    
    augment_time = time.time() - augment_start
    print(f"[RAG]   å¢å¼ºè€—æ—¶: {augment_time:.2f}s")
    print(f"[RAG]   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
    
    # ===== æ­¥éª¤ 3: Generation (ç”Ÿæˆ) =====
    print(f"[RAG] æ­¥éª¤ 3/3: LLM ç”Ÿæˆ")
    generation_start = time.time()
    
    recommendation = await self.generate_text(
        prompt=prompt,
        temperature=0.7,
        max_tokens=200
    )
    
    generation_time = time.time() - generation_start
    print(f"[RAG]   ç”Ÿæˆè€—æ—¶: {generation_time:.2f}s")
    
    # ===== å®Œæˆ =====
    total_time = time.time() - start_time
    print(f"[RAG] âœ… æ€»è€—æ—¶: {total_time:.2f}s")
    print(f"[RAG]   - æ£€ç´¢: {retrieval_time:.2f}s ({retrieval_time/total_time*100:.0f}%)")
    print(f"[RAG]   - å¢å¼º: {augment_time:.2f}s ({augment_time/total_time*100:.0f}%)")
    print(f"[RAG]   - ç”Ÿæˆ: {generation_time:.2f}s ({generation_time/total_time*100:.0f}%)")
    print(f"[RAG] ====== æµç¨‹å®Œæˆ ======")
    
    return recommendation
```

**æ€§èƒ½åˆ†æ**ï¼š
```
[RAG] ====== æµç¨‹å¼€å§‹ ======
[RAG] æ­¥éª¤ 1/3: æ£€ç´¢ç›¸å…³æ—¥è®°
[RAG]   æ£€ç´¢è€—æ—¶: 2.34s
[RAG]   æ‰¾åˆ° 3 ç¯‡ç›¸å…³æ—¥è®°
[RAG] æ­¥éª¤ 2/3: æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡
[RAG]   å¢å¼ºè€—æ—¶: 0.01s
[RAG]   æç¤ºè¯é•¿åº¦: 1245 å­—ç¬¦
[RAG] æ­¥éª¤ 3/3: LLM ç”Ÿæˆ
[RAG]   ç”Ÿæˆè€—æ—¶: 6.78s
[RAG] âœ… æ€»è€—æ—¶: 9.13s
[RAG]   - æ£€ç´¢: 2.34s (26%)
[RAG]   - å¢å¼º: 0.01s (0%)
[RAG]   - ç”Ÿæˆ: 6.78s (74%)
[RAG] ====== æµç¨‹å®Œæˆ ======
```

---

## ğŸ“Š ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–

### 4.1 è¯„ä¼°æŒ‡æ ‡

**èŒä½è¦æ±‚**ï¼š
> Model evaluation â€“ establish repeatable benchmarks, offline/online metrics

```python
# backend/app/services/evaluation.py
from typing import List, Dict
import json
from datetime import datetime

class RAGEvaluator:
    """
    RAG ç³»ç»Ÿè¯„ä¼°å·¥å…·
    """
    
    def __init__(self):
        self.metrics_log = []
    
    async def evaluate_retrieval(
        self,
        query: str,
        retrieved_docs: List[dict],
        ground_truth_ids: List[str] = None
    ) -> Dict[str, float]:
        """
        è¯„ä¼°æ£€ç´¢è´¨é‡
        
        æŒ‡æ ‡:
        - Precision@K: æ£€ç´¢ç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
        - Recall@K: ç›¸å…³æ–‡æ¡£ä¸­è¢«æ£€ç´¢åˆ°çš„æ¯”ä¾‹
        - MRR: Mean Reciprocal Rank
        """
        metrics = {}
        
        # 1. æ£€ç´¢æ•°é‡
        metrics['num_retrieved'] = len(retrieved_docs)
        
        # 2. å¹³å‡ç›¸ä¼¼åº¦
        certainties = [
            doc.get('_additional', {}).get('certainty', 0)
            for doc in retrieved_docs
        ]
        metrics['avg_certainty'] = sum(certainties) / len(certainties) if certainties else 0
        
        # 3. å¦‚æœæœ‰ ground truthï¼Œè®¡ç®— Precision/Recall
        if ground_truth_ids:
            retrieved_ids = [doc['diaryId'] for doc in retrieved_docs]
            true_positives = len(set(retrieved_ids) & set(ground_truth_ids))
            
            metrics['precision@k'] = true_positives / len(retrieved_ids) if retrieved_ids else 0
            metrics['recall@k'] = true_positives / len(ground_truth_ids) if ground_truth_ids else 0
        
        # è®°å½•
        self.metrics_log.append({
            'timestamp': datetime.now().isoformat(),
            'query': query,
            'metrics': metrics
        })
        
        return metrics
    
    async def evaluate_generation(
        self,
        generated_text: str,
        reference_text: str = None
    ) -> Dict[str, float]:
        """
        è¯„ä¼°ç”Ÿæˆè´¨é‡
        
        æŒ‡æ ‡:
        - Length: ç”Ÿæˆé•¿åº¦
        - Perplexity: å›°æƒ‘åº¦ï¼ˆéœ€è¦æ¨¡å‹æ”¯æŒï¼‰
        - BLEU: ä¸å‚è€ƒæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        metrics = {}
        
        # 1. åŸºç¡€æŒ‡æ ‡
        metrics['length'] = len(generated_text)
        metrics['num_sentences'] = generated_text.count('ã€‚') + generated_text.count('!')
        
        # 2. å¦‚æœæœ‰å‚è€ƒæ–‡æœ¬ï¼Œè®¡ç®— BLEU
        if reference_text:
            # ç®€åŒ–çš„ word overlap
            gen_words = set(generated_text)
            ref_words = set(reference_text)
            overlap = len(gen_words & ref_words)
            metrics['word_overlap'] = overlap / len(ref_words) if ref_words else 0
        
        return metrics
    
    def save_metrics(self, filepath: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.metrics_log, f, ensure_ascii=False, indent=2)
        
        print(f"[Eval] âœ… æŒ‡æ ‡å·²ä¿å­˜åˆ° {filepath}")
```

---

### 4.2 A/B æµ‹è¯•æ¡†æ¶

```python
# backend/app/services/ab_testing.py
import random
from typing import Dict, Callable

class ABTester:
    """
    A/B æµ‹è¯•æ¡†æ¶
    
    ç”¨äºæ¯”è¾ƒä¸åŒçš„:
    - æç¤ºè¯æ¨¡æ¿
    - æ¨¡å‹å‚æ•°
    - æ£€ç´¢ç­–ç•¥
    """
    
    def __init__(self):
        self.experiments = {}
        self.results = {}
    
    def create_experiment(
        self,
        name: str,
        variant_a: Callable,
        variant_b: Callable,
        traffic_split: float = 0.5
    ):
        """
        åˆ›å»º A/B æµ‹è¯•
        
        Args:
            name: å®éªŒåç§°
            variant_a: å˜ä½“ A çš„å‡½æ•°
            variant_b: å˜ä½“ B çš„å‡½æ•°
            traffic_split: æµé‡åˆ†é…ç»™ B çš„æ¯”ä¾‹
        """
        self.experiments[name] = {
            'variant_a': variant_a,
            'variant_b': variant_b,
            'traffic_split': traffic_split
        }
        self.results[name] = {'a': [], 'b': []}
    
    async def run_experiment(
        self,
        name: str,
        *args,
        **kwargs
    ):
        """
        è¿è¡Œ A/B æµ‹è¯•
        """
        experiment = self.experiments[name]
        
        # éšæœºåˆ†é…
        if random.random() < experiment['traffic_split']:
            variant = 'b'
            result = await experiment['variant_b'](*args, **kwargs)
        else:
            variant = 'a'
            result = await experiment['variant_a'](*args, **kwargs)
        
        # è®°å½•ç»“æœ
        self.results[name][variant].append(result)
        
        return result, variant
    
    def analyze_results(self, name: str) -> Dict:
        """
        åˆ†æ A/B æµ‹è¯•ç»“æœ
        """
        results_a = self.results[name]['a']
        results_b = self.results[name]['b']
        
        analysis = {
            'variant_a': {
                'count': len(results_a),
                'avg_length': sum(len(r) for r in results_a) / len(results_a) if results_a else 0
            },
            'variant_b': {
                'count': len(results_b),
                'avg_length': sum(len(r) for r in results_b) / len(results_b) if results_b else 0
            }
        }
        
        return analysis


# ä½¿ç”¨ç¤ºä¾‹
ab_tester = ABTester()

# å®éªŒï¼šæ¯”è¾ƒä¸¤ç§æç¤ºè¯æ¨¡æ¿
async def prompt_v1(content):
    prompt = f"è¯·ä¸ºè¿™ç¯‡æ—¥è®°æä¾›å»ºè®®ï¼š{content}"
    return await llama_service.generate_text(prompt)

async def prompt_v2(content):
    prompt = f"ä½œä¸ºä¸€ä¸ªæ—¥è®°åŠ©æ‰‹ï¼Œè¯·åˆ†æè¿™ç¯‡æ—¥è®°å¹¶æä¾›æ·±å…¥çš„å»ºè®®ï¼š{content}"
    return await llama_service.generate_text(prompt)

ab_tester.create_experiment(
    name="prompt_comparison",
    variant_a=prompt_v1,
    variant_b=prompt_v2,
    traffic_split=0.5
)

# è¿è¡Œå®éªŒ
result, variant = await ab_tester.run_experiment(
    "prompt_comparison",
    content="ä»Šå¤©å¤©æ°”å¾ˆå¥½"
)

print(f"ä½¿ç”¨å˜ä½“: {variant}")
print(f"ç»“æœ: {result}")
```

---

## ğŸš€ ç¬¬äº”éƒ¨åˆ†ï¼šä»åŸå‹åˆ°ç”Ÿäº§

### 5.1 å¼€å‘æµç¨‹

**èŒä½è¦æ±‚**ï¼š
> Prototyping â†’ Production â€“ craft PoCs in notebooks, then convert to clean, tested services

**é˜¶æ®µ 1: Jupyter Notebook åŸå‹**

```python
# notebooks/rag_prototype.ipynb

# 1. å¿«é€ŸéªŒè¯æƒ³æ³•
import httpx
import asyncio

async def test_ollama():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3.2:1b",
                "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»è‡ªå·±",
                "stream": False
            }
        )
        return response.json()

result = await test_ollama()
print(result['response'])

# 2. æµ‹è¯•ä¸åŒçš„æç¤ºè¯
prompts = [
    "è¯·ç®€çŸ­å›å¤ï¼š{content}",
    "ä½œä¸ºä¸“å®¶ï¼Œè¯·åˆ†æï¼š{content}",
    "ç”¨æ¸©æš–çš„è¯­æ°”å›å¤ï¼š{content}"
]

for prompt_template in prompts:
    # æµ‹è¯•æ¯ä¸ªæ¨¡æ¿...
    pass

# 3. å¯è§†åŒ–ç»“æœ
import matplotlib.pyplot as plt
plt.plot(response_times)
plt.title('ä¸åŒæç¤ºè¯çš„å“åº”æ—¶é—´')
plt.show()
```

**é˜¶æ®µ 2: è½¬æ¢ä¸ºç”Ÿäº§ä»£ç **

```python
# backend/app/services/llama_rag_service.py

class LlamaRAGService:
    """
    ä» notebook è½¬æ¢çš„ç”Ÿäº§ä»£ç 
    
    æ”¹è¿›:
    - æ·»åŠ é”™è¯¯å¤„ç†
    - æ·»åŠ æ—¥å¿—
    - æ·»åŠ ç±»å‹æç¤º
    - æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²
    - æ·»åŠ å•å…ƒæµ‹è¯•
    """
    
    async def generate_text(self, prompt: str) -> str:
        """è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²"""
        try:
            # ç”Ÿäº§çº§å®ç°
            pass
        except Exception as e:
            # å®Œå–„çš„é”™è¯¯å¤„ç†
            logger.error(f"Error: {e}")
            raise

# å•å…ƒæµ‹è¯•
# tests/test_llama_rag_service.py

import pytest

@pytest.mark.asyncio
async def test_generate_text():
    service = LlamaRAGService()
    result = await service.generate_text("æµ‹è¯•")
    assert len(result) > 0
    assert isinstance(result, str)

@pytest.mark.asyncio
async def test_generate_text_timeout():
    service = LlamaRAGService()
    service.timeout = 0.001  # æçŸ­è¶…æ—¶
    
    with pytest.raises(httpx.TimeoutException):
        await service.generate_text("æµ‹è¯•" * 1000)
```

---

### 5.2 æœåŠ¡åŒ–éƒ¨ç½²

```python
# backend/app/main.py
from fastapi import FastAPI
from app.api.routes import diaries

app = FastAPI(
    title="AI Diary API",
    description="Production LLM service",
    version="1.0.0"
)

# æ³¨å†Œè·¯ç”±
app.include_router(diaries.router, prefix="/diaries", tags=["diaries"])

# å¥åº·æ£€æŸ¥
@app.get("/health")
async def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹
    
    æ£€æŸ¥:
    - API è¿è¡ŒçŠ¶æ€
    - Ollama è¿æ¥
    - Weaviate è¿æ¥
    """
    health_status = {
        "status": "healthy",
        "services": {}
    }
    
    # æ£€æŸ¥ Ollama
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{settings.ollama_url}/api/tags")
            health_status["services"]["ollama"] = "healthy" if response.status_code == 200 else "unhealthy"
    except:
        health_status["services"]["ollama"] = "unhealthy"
    
    # æ£€æŸ¥ Weaviate
    try:
        weaviate_client.schema.get()
        health_status["services"]["weaviate"] = "healthy"
    except:
        health_status["services"]["weaviate"] = "unhealthy"
    
    return health_status
```

---

## ğŸ”¬ ç¬¬å…­éƒ¨åˆ†ï¼šå¤šè¯­è¨€æ”¯æŒ (JA-EN)

### 6.1 å¤šè¯­è¨€ Embeddings

**èŒä½è¦æ±‚**ï¼š
> Experience fine-tuning or distilling language models, especially for multilingual tasks (JA-EN)

```python
async def generate_multilingual_embedding(
    self,
    text: str,
    language: str = "auto"
) -> List[float]:
    """
    ç”Ÿæˆå¤šè¯­è¨€åµŒå…¥
    
    Llama 3.2 æ”¯æŒ:
    - è‹±è¯­ (EN)
    - æ—¥è¯­ (JA)
    - ä¸­æ–‡ (ZH)
    - è‡ªåŠ¨æ£€æµ‹
    """
    # è¯­è¨€æ ‡è®°
    if language == "auto":
        # ç®€å•çš„è¯­è¨€æ£€æµ‹
        if any('\u4e00' <= char <= '\u9fff' for char in text):
            language = "zh"
        elif any('\u3040' <= char <= '\u309f' or '\u30a0' <= char <= '\u30ff' for char in text):
            language = "ja"
        else:
            language = "en"
    
    print(f"[Embeddings] æ£€æµ‹åˆ°è¯­è¨€: {language}")
    
    # ç”ŸæˆåµŒå…¥
    embedding = await self.generate_embedding(text)
    
    return embedding

# è·¨è¯­è¨€æœç´¢ç¤ºä¾‹
async def cross_lingual_search(self, query: str, user_id: str):
    """
    è·¨è¯­è¨€æœç´¢
    
    ç¤ºä¾‹:
    - æŸ¥è¯¢ (EN): "sunny day"
    - ç»“æœ (JA): "æ™´ã‚ŒãŸæ—¥"
    - ç»“æœ (ZH): "é˜³å…‰æ˜åªš"
    """
    # Llama 3.2 çš„åµŒå…¥æ˜¯è·¨è¯­è¨€å¯¹é½çš„
    # è‹±æ–‡æŸ¥è¯¢å¯ä»¥åŒ¹é…æ—¥æ–‡/ä¸­æ–‡æ–‡æ¡£
    
    results = await self.search_similar_diaries(
        user_id=user_id,
        query_text=query,
        limit=10
    )
    
    return results
```

---

## ğŸ“ˆ ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç›‘æ§ä¸å¯è§‚æµ‹æ€§

### 7.1 æ€§èƒ½ç›‘æ§

```python
# backend/app/middleware/monitoring.py
from fastapi import Request
import time
import logging

logger = logging.getLogger(__name__)

@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    """
    ç›‘æ§æ¯ä¸ªè¯·æ±‚çš„å¤„ç†æ—¶é—´
    """
    start_time = time.time()
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    
    # è®°å½•æ…¢è¯·æ±‚
    if process_time > 10.0:
        logger.warning(
            f"Slow request: {request.method} {request.url.path} "
            f"took {process_time:.2f}s"
        )
    
    return response
```

---

## ğŸ“ å­¦ä¹ è·¯å¾„

### åˆçº§ (1-2 å‘¨)
1. è¿è¡Œé¡¹ç›®ï¼Œç†è§£ RAG æµç¨‹
2. ä¿®æ”¹æç¤ºè¯ï¼Œè§‚å¯Ÿè¾“å‡ºå˜åŒ–
3. è°ƒæ•´å‚æ•° (temperature, max_tokens)

### ä¸­çº§ (1-2 æœˆ)
1. å®ç°æ–°çš„æ£€ç´¢ç­–ç•¥ (æ··åˆæœç´¢)
2. æ·»åŠ è¯„ä¼°æŒ‡æ ‡
3. å°è¯•ä¸åŒçš„ LLM æ¨¡å‹

### é«˜çº§ (2-3 æœˆ)
1. Fine-tune æ¨¡å‹
2. å®ç° function calling
3. æ„å»º multi-agent ç³»ç»Ÿ

---

## ğŸ’¼ å¦‚ä½•å±•ç¤ºæ­¤é¡¹ç›®

### ç®€å†æè¿°

```
AI æ—¥è®°ç³»ç»Ÿ - RAG + è¯­ä¹‰æœç´¢
æŠ€æœ¯æ ˆ: Ollama (Llama 3.2), Weaviate, FastAPI, Docker

â€¢ è®¾è®¡å¹¶éƒ¨ç½²å®Œæ•´çš„ RAG ç®¡é“ï¼šEmbeddings â†’ Weaviate å‘é‡æœç´¢ â†’ LLM ç”Ÿæˆ
â€¢ å®ç°æœ¬åœ° LLM æ¨ç†æœåŠ¡ï¼šOllama (1B æ¨¡å‹)ï¼Œå»¶è¿Ÿ < 10sï¼Œ100% æœ¬åœ°è¿è¡Œ
â€¢ æ„å»ºè¯­ä¹‰æœç´¢ç³»ç»Ÿï¼šä½¿ç”¨ HNSW ç®—æ³•ï¼ŒO(log N) å¤æ‚åº¦ï¼Œæ”¯æŒå¤šè¯­è¨€ (JA-EN-ZH)
â€¢ æ€§èƒ½ä¼˜åŒ–ï¼šæ¨¡å‹é‡åŒ–ã€æ‰¹å¤„ç†ã€å¼‚æ­¥å¤„ç†ï¼Œæå‡ååé‡ 3x
â€¢ å»ºç«‹è¯„ä¼°ä½“ç³»ï¼šPrecision@K, Recall@K, å»¶è¿Ÿç›‘æ§ï¼ŒA/B æµ‹è¯•æ¡†æ¶
```

---

## âœ… æ€»ç»“

æœ¬é¡¹ç›®å®Œæ•´å±•ç¤ºäº† Monoya AI/ML Engineer æ‰€éœ€æŠ€èƒ½ï¼š

âœ… **LLM**: Ollama ç”Ÿäº§éƒ¨ç½²  
âœ… **Embeddings**: å‘é‡ç”Ÿæˆä¸ä¼˜åŒ–  
âœ… **Vector DB**: Weaviate è¯­ä¹‰æœç´¢  
âœ… **RAG**: ç«¯åˆ°ç«¯ pipeline  
âœ… **Serving**: FastAPI + Docker  
âœ… **Evaluation**: æŒ‡æ ‡ä½“ç³»ä¸ A/B æµ‹è¯•  
âœ… **Multilingual**: JA-EN-ZH æ”¯æŒ  
âœ… **Production**: ç›‘æ§ã€æ—¥å¿—ã€é”™è¯¯å¤„ç†  

**ç›¸å…³æ–‡æ¡£**ï¼š
- [RAG Flow Explained](./RAG_FLOW_EXPLAINED.md)
- [Llama Setup](./LLAMA_SETUP.md)
- [Architecture](./ARCHITECTURE_CN.md)

